---
description: Universal intelligence-driven LoopFlow workflow orchestrator. Reads constitution + project context to derive requirements automatically. Chains /sp.specify â†’ /sp.plan â†’ /sp.tasks â†’ /sp.implement with quality gates. Works for ANY chapter, feature, or task (not limited to Python).
---

# /sp.loopflow: Universal LoopFlow Orchestrator

**Purpose**: Execute the complete LoopFlow+ workflow (Evals â†’ Spec â†’ Plan â†’ Tasks â†’ Implement â†’ Validate) for ANY chapter, feature, or task using **vertical intelligence** (constitution + project context + domain skills). Goal-oriented, context-adaptive, and quality-assured.

**Intelligence Sources**:
- Constitution: Project vision, core principles, target audience, philosophies
- Project Context: Chapter index, book structure, existing specs
- Domain Skills: Available skills library (`.claude/skills/`)
- User Intent: Natural language description of what to build

**Adaptive Workflow**: 0-5 targeted questions based on what intelligence can't derive. NO hardcoded questionnaires.

## User Input

```text
$ARGUMENTS
```

---

## âš ï¸ CRITICAL ANTI-PATTERN: DON'T OVER-ENGINEER WITH AI

### Constitutional Alignment

âœ… **Principle 2 (Co-Learning Partner):** AI used strategically, not blindly for everything
âœ… **Core Philosophy #1 (AI Spectrum):** Teaching when Assisted vs. Driven vs. Native makes sense
âœ… **Graduated Teaching (Principle 13):** Direct foundations first, AI for complexity
âœ… **"Specs Are the New Syntax":** Focus on high-value specification work, not trivial command execution

---

## ğŸ­ SUPER ORCHESTRA MODE (Optional Deep-Research)

**When to invoke**: Task requires comprehensive intelligence gathering + market-defining output

**Indicators**:
- User mentions "research Context7" or "gather from official sources"
- Gap identified that spans multiple scattered documentation sources
- Output must surpass market alternatives (not just meet internal specs)
- Strategic positioning required ("Is this better than official docs?")

**If triggered, apply**:
- Use `super-orchestra` agent
- Use `super-orchestra-session` output style
- Context7 library research (8000+ tokens)
- WebFetch official sources (3+ URLs)
- Iterative refinement with positioning validation
- Meta-learning capture for system evolution

**Example**: Chapter 5 Skills/Plugins/MCP session (see `.claude/agents/super-orchestra.md`)

---

## PHASE 0: INTELLIGENT CONTEXT DISCOVERY

**STEP 1: Read Authoritative Sources**

Immediately read these files FIRST (no user questions yet):

1. **Constitution** (`.specify/memory/constitution.md`):
   - Project vision: "Specs Are the New Syntax" (AI-native development)
   - Core principles (18 total, especially Principle 13: Graduated Teaching, Principle 18: Three Roles)
   - Core philosophies (8 total, especially Evals-First, Co-Learning, Spec-First, Validation-First)
   - Target audience: Aspiring/Professional/Founders with graduated complexity
   - Nine Pillars: AI CLI, Markdown, MCP, AI-First IDEs, Cross-Platform, TDD, SDD, Composable Skills, Cloud-Native
   - **CRITICAL**: "AI Development Spectrum" (Assisted 2-3x â†’ Driven 5-10x â†’ Native 50-99x)

2. **Chapter Index** (IF book chapter) (`specs/book/chapter-index.md`):
   - Chapter number â†’ Part mapping
   - Prerequisites (what chapters must come before)
   - Complexity tier (A1-C2 CEFR levels)
   - Status (planned/in-progress/completed)

3. **Existing Specs** (`specs/` directory):
   - Similar chapters/features for pattern reference
   - Naming conventions
   - Structure examples

4. **Domain Skills** (`.claude/skills/`):
   - Available skills for this task type
   - Teaching patterns (if educational content)
   - Validation strategies (if technical feature)

**STEP 2: Automatic Derivations**

From sources above, derive WITHOUT asking user:

**For Book Chapters:**
- Part number (from chapter number range in chapter-index)
- Audience tier: Part 1-3 (Aspiring/Beginner A1-A2) â†’ Part 4-5 (Intermediate B1-B2) â†’ Part 6-8 (Advanced C1) â†’ Part 9-13 (Professional C2)
- Complexity level and cognitive load limits
- Prerequisites (chapters that must exist first)
- Relevant domain skills (learning-objectives, concept-scaffolding, code-example-generator, etc.)
- Teaching pattern (Graduated Teaching Principle 13: Book teaches stable â†’ AI handles complex â†’ AI orchestrates scale)

**For Code Features:**
- Task type (authentication, API, database, deployment, etc.)
- Audience (professional developers)
- Prerequisites (existing codebase components)
- Relevant domain skills (technical-clarity, assessment-builder, etc.)

**For Documentation/Testing:**
- Scope from file paths and existing content
- Audience (contributors, users, developers)
- Prerequisites (what must exist to document/test)

**CRITICAL ANTI-PATTERN CHECK:**
- Is this task a simple, deterministic command? (like `uv init`, `docker build`, `git commit`)
  - âœ… IF YES: Don't create elaborate AI workflows. Document direct commands.
  - âœ… IF NO: This is where AI adds value (complex workflows, troubleshooting, strategic decisions).

**STEP 3: Apply Constitution Context**

Encode these principles into ALL downstream phases:

**From Core Philosophy:**
1. **Evals-First Development**: Define success criteria BEFORE specs
2. **Co-Learning Partnership**: Bidirectional learning (AI teaches student, student teaches AI feedback loop)
3. **Spec-First Development**: "Specs Are the New Syntax" - articulating intent is the primary skill
4. **Validation-First Safety**: Never trust, always verify

**From Graduated Teaching (Principle 13):**
- **Tier 1 (Book Content)**: Teach stable, foundational concepts that don't change often
- **Tier 2 (AI Companion)**: AI handles complex execution from specifications
- **Tier 3 (AI Orchestration)**: AI orchestrates multi-step workflows at scale (10+ items)

**WHEN TO USE AI (Critical Distinction):**
- âŒ **DON'T use AI for**: Simple commands (`npm install`, `uv add`, `git status`), straightforward operations, 1-step tasks
- âœ… **DO use AI for**: Troubleshooting, debugging complex errors, understanding concepts, strategic decisions, multi-step workflows

**From AI Development Spectrum:**
- **Assisted (2-3x)**: AI as helper for simple tasks (Parts 1-2)
- **Driven (5-10x)**: AI generates from specs (Parts 3-8) â† Primary focus
- **Native (50-99x)**: AI as core product capability (Parts 9-13)

**From Target Audience:**
- **Aspiring (A1-A2)**: Max 7 concepts per section, 2 options max, cognitive load managed
- **Intermediate (B1-B2)**: 7 concepts per section, 3-4 options, tradeoff discussions
- **Advanced (C1)**: 10 concepts per section, 5+ options, architecture patterns
- **Professional (C2)**: No artificial limits, production complexity

**STEP 4: Identify Genuine Ambiguities**

Now that you have full context, identify ONLY what's genuinely ambiguous (0-5 questions max):

**Ask IF**:
- Existing context found â†’ "Use existing [spec/approach] or start fresh?"
- Goal is broad/vague â†’ "What specific aspect to emphasize?"
- Multiple valid approaches â†’ "Which strategy fits best: [A, B, or C]?"
- Capstone vs conceptual unclear â†’ "Should students BUILD something hands-on?"
- Scope ambiguous â†’ "What's in scope vs out of scope?"

**DON'T Ask IF**:
- Constitution already specifies it (audience tier, complexity, principles)
- Chapter index already defines it (prerequisites, part number)
- Task type is obvious from goal ("Add authentication" = code feature)
- Direct command is appropriate (simple `uv init` doesn't need AI workflow)

**Output**: Intelligence object containing:
```json
{
  "task_type": "book_chapter | code_feature | documentation | testing | refactoring",
  "audience_tier": "aspiring | intermediate | advanced | professional",
  "complexity_level": "A1 | A2 | B1 | B2 | C1 | C2",
  "prerequisites": ["chapter-11", "chapter-12"],
  "domain_skills": ["learning-objectives", "concept-scaffolding"],
  "teaching_pattern": "direct_commands | ai_companion | ai_orchestration",
  "ai_usage_strategy": "describe when AI adds value vs direct commands",
  "cognitive_load_limit": 7,
  "sandbox_validation_required": true,
  "commands_to_test": ["list all CLI commands students will run"],
  "ambiguities_clarified": {"question": "answer"}
}
```

---

## PHASE 1: SPECIFICATION + CLARIFICATION GATE

```
â†’ Invoke: /sp.specify [intelligence-context]
  â”œâ”€ Pass: Full intelligence object from Phase 0
  â”œâ”€ Apply: Evals-first (success criteria BEFORE implementation)
  â”œâ”€ Apply: AI usage strategy (when to use AI vs direct commands)
  â”œâ”€ Apply: Graduated teaching pattern (Tier 1/2/3 mapping)
  â”œâ”€ Create: specs/[feature-slug]/spec.md
  â””â”€ Report: "Spec created with evals, AI strategy, and teaching tiers defined."

â†’ Invoke: /sp.clarify (Quality Gate)
  â”œâ”€ Read: specs/[feature-slug]/spec.md
  â”œâ”€ Identify: Underspecified areas, missing evals, ambiguous AI usage
  â”œâ”€ Check: Is this over-engineering simple tasks with AI?
  â”œâ”€ Ask: Up to 5 targeted clarification questions
  â”œâ”€ Update: spec.md with answers encoded
  â””â”€ Report: "Spec clarified. AI usage strategy validated."

â†’ Create Feature Branch (AFTER spec exists)
  â”œâ”€ Derive branch name from spec directory (e.g., specs/part-4-chapter-15/ â†’ part-4-chapter-15)
  â”œâ”€ Check current branch:
  â”‚   IF current == main â†’ Create new branch matching spec directory
  â”‚   IF current == spec directory â†’ Stay on it
  â”‚   IF current != spec directory â†’ Warn and ask user to switch
  â”œâ”€ Execute: git checkout -b [spec-directory-name] (only if on main)
  â””â”€ Report: "âœ… Branch: [branch-name]"

WAIT: User reviews spec.md
â†’ User confirms: "âœ… Spec approved" or provides feedback
  â”œâ”€ If feedback: Update spec.md iteratively (may re-run /sp.clarify)
  â””â”€ If approved: Continue to PHASE 2
```

**Spec Must Include**:
- **Evals Section**: Success criteria defined FIRST (before any implementation details)
- **AI Usage Strategy**: Clear distinction between direct commands vs AI collaboration
  - Example: "Students run `uv init` directly (1 second). Use AI for troubleshooting dependency conflicts."
- **Teaching Tiers** (if book chapter):
  - Tier 1: What concepts book teaches directly
  - Tier 2: When AI companion handles complexity
  - Tier 3: When AI orchestrates multi-step workflows
- **Duration**: Realistic time estimates (not inflated)
  - Example: "Installation takes 1 minute, not 45 minutes"
- **Cognitive Load**: Respects audience tier limits (A2 = max 7 concepts)

**Anti-Pattern Detection**:
- âŒ If spec says "Use AI to run `docker build`" â†’ FLAG: This is over-engineering
- âŒ If duration is 50+ minutes for simple operations â†’ FLAG: Inflated estimate
- âŒ If every task involves "Ask AI to..." â†’ FLAG: Not teaching strategic AI use

---

## PHASE 2: PLANNING + ADR GATE

```
â†’ Invoke: /sp.plan [spec-context]
  â”œâ”€ Read: specs/[feature-slug]/spec.md (clarified)
  â”œâ”€ Apply: Teaching pattern (direct commands vs AI collaboration)
  â”œâ”€ Apply: Proficiency levels (CEFR if book chapter)
  â”œâ”€ Apply: Constitutional principles (Graduated Teaching, Co-Learning)
  â”œâ”€ Create: specs/[feature-slug]/plan.md
  â””â”€ Report: "Plan created with teaching strategy and AI usage mapped."

â†’ Invoke: /sp.adr (Architectural Decision Gate)
  â”œâ”€ Read: specs/[feature-slug]/plan.md
  â”œâ”€ Detect: Architecturally significant decisions
  â”œâ”€ Suggest: "ğŸ“‹ Decision detected: [X]. Document with /sp.adr [title]?"
  â”œâ”€ Wait: User consent (never auto-create)
  â”œâ”€ Create: history/adr/[NNN]-[decision-title].md (if approved)
  â””â”€ Report: "ADR created and linked." OR "Suggestion noted."

WAIT: User reviews plan.md (+ any ADRs)
â†’ User confirms: "âœ… Plan approved" or provides feedback
  â”œâ”€ If feedback: Update plan.md iteratively
  â””â”€ If approved: Continue to PHASE 3
```

**Plan Must Include**:
- **Direct Commands Section**: List all commands students run directly (with timing)
  - Example: "`uv init my-project` (1 second), `uv add requests` (1-3 seconds)"
- **AI Collaboration Section**: When/why to use AI (strategic, not everything)
  - Example: "Use AI for: understanding pyproject.toml structure, troubleshooting version conflicts, explaining virtual environments"
- **Lesson Structure** (if book chapter):
  - Estimated reading time (realistic)
  - "Try with AI" prompts (3-4 focused, not 8+ verbose)
  - Chapter 1 format: `### Prompt N: Title` â†’ code block â†’ `**Expected outcome:**` description

---

## PHASE 3: TASKS + ANALYSIS GATE

```
â†’ Invoke: /sp.tasks [spec+plan-context]
  â”œâ”€ Read: specs/[feature-slug]/spec.md + plan.md
  â”œâ”€ Apply: Direct commands vs AI workflow mapping
  â”œâ”€ Apply: Acceptance criteria from evals
  â”œâ”€ Create: specs/[feature-slug]/tasks.md
  â””â”€ Report: "Tasks created with clear AI usage boundaries."

â†’ Invoke: /sp.analyze (Cross-Artifact Consistency Gate)
  â”œâ”€ Read: specs/[feature-slug]/{spec,plan,tasks}.md
  â”œâ”€ Validate: Objectives â†’ plan â†’ tasks traceability
  â”œâ”€ Check: AI usage strategy consistency (not over-engineering)
  â”œâ”€ Detect: Missing tasks, orphaned objectives, scope drift
  â”œâ”€ Report: Issues (critical/major/minor) + recommendations
  â””â”€ Output: analysis-report.md

WAIT: User reviews tasks.md + analysis report
â†’ User confirms: "âœ… Tasks approved" or provides feedback
  â”œâ”€ If critical issues: Must fix before proceeding
  â”œâ”€ If major issues: Should fix (user decision)
  â”œâ”€ If minor issues: Nice to fix (user decision)
  â””â”€ If approved: Continue to PHASE 4
```

**Task Anti-Pattern Checks**:
- âŒ "Create AI prompt for running `npm install`" â†’ Should be "Run `npm install` directly"
- âŒ "Write 50-minute lesson for 1-minute operation" â†’ Should be realistic duration
- âŒ "8 verbose 'Try with AI' prompts" â†’ Should be 3-4 focused prompts

---

## PHASE 4: IMPLEMENTATION + VALIDATION GATE

```
â†’ Invoke: /sp.implement [feature-slug]
  â”œâ”€ Read: specs/[feature-slug]/{spec,plan,tasks}.md (all approved)
  â”œâ”€ Strategy: Task-type dependent (lessons/code/tests/docs)
  â”œâ”€ Invoke: Appropriate subagent with FULL context including:
  â”‚   - Intelligence object (audience, complexity, prerequisites)
  â”‚   - AI usage strategy (direct commands vs collaboration)
  â”‚   - Teaching pattern (Tier 1/2/3 if applicable)
  â”‚   - Constitutional principles (Co-Learning, Graduated Teaching)
  â”‚   - Anti-pattern warnings (don't over-engineer with AI)
  â”œâ”€ Create: Implementation artifacts (lessons, code, tests, etc.)
  â””â”€ Report: "Implementation complete. Reviewing for AI over-engineering..."

â†’ Validation Review (Conceptual)
  â”œâ”€ For book chapters:
  â”‚   - Duration realistic? (not inflated for simple operations)
  â”‚   - Direct commands documented clearly? (not hidden behind AI)
  â”‚   - "Try with AI" section uses Chapter 1 format? (3-4 focused prompts)
  â”‚   - AI usage strategic? (troubleshooting, understanding, not trivial commands)
  â”‚   - Line count reasonable? (not verbose explanations of simple operations)
  â”œâ”€ For code features:
  â”‚   - Tests pass?
  â”‚   - Code quality meets standards?
  â”‚   - Documentation clear?
  â”œâ”€ Invoke: technical-reviewer + proof-validator (if applicable)
  â””â”€ Report: PASS / CONDITIONAL PASS / FAIL with conceptual issues

â†’ If CONDITIONAL PASS or FAIL:
  â”œâ”€ Apply fixes for critical issues
  â”œâ”€ Re-run validation
  â””â”€ Repeat until conceptual validation PASS

â†’ Sandbox Validation (Hands-On Testing) **CRITICAL**
  â”œâ”€ Philosophy: "If you have not run anything in sandbox, chances are it won't work"
  â”œâ”€ For book chapters with hands-on commands:
  â”‚   - Extract ALL commands students will run
  â”‚   - Test EVERY command in actual environment
  â”‚   - Verify command syntax (CLI vs session commands)
  â”‚   - Verify output matches lesson claims
  â”‚   - Test "Try With AI" prompts for achievability
  â”‚   - Document what actually works vs what's documented
  â”œâ”€ For code features:
  â”‚   - Run full test suite in sandbox
  â”‚   - Execute code in target environment
  â”‚   - Verify deployment steps work end-to-end
  â”‚   - Test edge cases and error paths
  â”œâ”€ Create: SANDBOX-AUDIT-REPORT.md with:
  â”‚   - Commands tested (with actual output)
  â”‚   - Errors found (with line numbers)
  â”‚   - Fixes applied (with evidence)
  â”‚   - Re-test results (verification)
  â””â”€ Report: SANDBOX PASS / SANDBOX FAIL with specific command errors

â†’ If SANDBOX FAIL:
  â”œâ”€ Apply fixes for ALL command syntax errors
  â”œâ”€ Re-run sandbox tests
  â”œâ”€ Update SANDBOX-AUDIT-REPORT.md with fix verification
  â””â”€ Repeat until SANDBOX PASS

WAIT: User reviews implementation + validation report + sandbox audit
â†’ User confirms: "âœ… Implementation approved"
  â””â”€ Proceed to PHASE 5
```

**Implementation Intelligence Context** (CRITICAL for subagents):

Pass this FULL context to subagents (lesson-writer, general-purpose, etc.):

```json
{
  "intelligence": { /* Phase 0 intelligence object */ },
  "ai_usage_strategy": "Direct commands for X, Y, Z. Use AI for A, B, C.",
  "teaching_pattern": "Tier 1: Book teaches [concepts]. Tier 2: AI handles [complexity]. Tier 3: AI orchestrates [scale].",
  "anti_patterns": [
    "Don't use AI for simple commands like `uv init`",
    "Don't inflate durations (1-minute tasks shouldn't be 45-minute lessons)",
    "Don't create 8+ verbose 'Try with AI' prompts (use 3-4 focused ones)",
    "Use Chapter 1 format: ### Prompt N: Title â†’ code block â†’ **Expected outcome:**"
  ],
  "constitutional_principles": [
    "Principle 13 (Graduated Teaching): Direct â†’ AI Companion â†’ AI Orchestration",
    "Principle 18 (Three Roles): AI as Teacher/Student/Co-Worker",
    "Core Philosophy #2 (Co-Learning): Bidirectional learning, not one-way instruction",
    "Core Philosophy #1 (AI Spectrum): Assisted â†’ Driven â†’ Native (teach when each applies)"
  ],
  "validation_criteria": {
    "duration_realistic": true,
    "direct_commands_clear": true,
    "ai_usage_strategic": true,
    "try_with_ai_format": "Chapter 1 clean format",
    "line_count_reasonable": true
  }
}
```

---

## PHASE 5: FINALIZATION + OPTIONAL GIT WORKFLOW

```
â†’ Update project tracking:
  â”œâ”€ For chapters: Update chapter-index.md status
  â”œâ”€ For features: Update feature tracking (if exists)
  â””â”€ Report: "Project tracking updated."

â†’ Optional: Git workflow
  â”œâ”€ User may request: "/sp.git.commit_pr" for automated commit + PR
  â”œâ”€ Or: Manual commit with summary
  â””â”€ Report: "Git workflow completed" OR "Manual commit required"

â†’ Create PHR (Prompt History Record):
  â”œâ”€ Document: User goal, intelligence derived, decisions, AI strategy applied
  â”œâ”€ Include: Anti-pattern checks performed, validation results
  â”œâ”€ Save: history/prompts/[feature-slug]/
  â””â”€ Report: "PHR created for future reference."
```

---

## KEY LESSONS FROM CHAPTER 12 UV REVIEW

### What We Fixed

**Problem**: Lessons over-engineered simple operations with AI:
- "Tell AI to install UV" (30-second command became 10+ minute AI conversation)
- "Ask AI to run `uv init`" (1-second command became elaborate prompt)
- 667-line lesson for 3 simple commands
- 8 verbose "Try with AI" prompts instead of 3-4 focused ones
- Duration: 45 minutes for 1-minute installation

**Solution Applied**:
- Direct commands for simple operations: "Run `uv init`" (not "Ask AI to run `uv init`")
- AI for complex problems: troubleshooting, understanding concepts, strategic decisions
- Realistic durations: 15 minutes for installation (actual time), not 45 minutes
- Clean "Try with AI" format: 3-4 focused prompts using Chapter 1 style
- Line count reduction: 38% overall (3,456 â†’ 2,144 lines)

**Results**:
- Students learned WHEN to use AI strategically (not for everything)
- Lessons became efficient, not verbose
- Constitutional alignment: Graduated Teaching (direct â†’ AI â†’ orchestration)

### Encode These Lessons

**In Specifications**:
- âœ… "Students run `uv init` directly (1 second)"
- âŒ "Students tell AI to initialize a project using UV"

**In Plans**:
- âœ… "Direct commands section: Installation (30 sec), Project creation (1 sec), Add dependency (1-3 sec)"
- âœ… "AI collaboration section: Troubleshooting PATH errors, understanding pyproject.toml, resolving version conflicts"
- âŒ "AI workflow for every command"

**In Implementations**:
- âœ… Chapter 1 "Try with AI" format (3-4 prompts, clean structure)
- âœ… Realistic durations matching actual time
- âŒ Verbose explanations of trivial operations
- âŒ 8+ "Try with AI" prompts with lengthy pre-explanations

---

## CRITICAL SUCCESS FACTORS

1. **Vertical Intelligence**: Constitution + project context read FIRST, questions SECOND
2. **Goal-Oriented**: User states GOAL, AI derives STRATEGY
3. **AI Usage Strategy**: Clear distinction between direct commands (simple) vs AI (complex)
4. **Graduated Teaching**: Apply Principle 13 (Book â†’ AI Companion â†’ AI Orchestration)
5. **Quality Gates**: Every phase prevents bad patterns from propagating
6. **Context Preservation**: Full intelligence + AI strategy passed through all phases
7. **Anti-Pattern Detection**: Flag over-engineering with AI for simple tasks
8. **Constitutional Compliance**: All outputs align with vision and principles
9. **Realistic Expectations**: Durations, line counts, and complexity match actual needs
10. **Shipping-Ready**: Built-in quality, not post-hoc validation

---

## REFERENCES

- **Constitution**: `.specify/memory/constitution.md` (source of truth)
- **Project Structure**:
  - Chapter Index: `specs/book/chapter-index.md`
  - Specs Directory: `specs/`
  - Skills Library: `.claude/skills/`
- **LoopFlow Commands**:
  - `/sp.specify` - Create specifications
  - `/sp.clarify` - Clarify underspecified areas
  - `/sp.plan` - Create implementation plans
  - `/sp.adr` - Document architectural decisions
  - `/sp.tasks` - Generate task checklists
  - `/sp.analyze` - Cross-artifact consistency
  - `/sp.implement` - Execute implementation
  - `/sp.git.commit_pr` - Git workflow automation

---

## ONE COMMAND. UNIVERSAL INTELLIGENCE. COMPLETE WORKFLOW.

Run `/sp.loopflow [goal]` and the system executes:

**PHASE 0: Intelligent Discovery** â†’ Constitution + context + AI usage strategy derived
**PHASE 1: Specification + Clarification** â†’ Evals-first + AI strategy + approval gate
**PHASE 2: Planning + ADR** â†’ Teaching pattern + direct vs AI mapping + approval gate
**PHASE 3: Tasks + Analysis** â†’ Anti-pattern checks + consistency + approval gate
**PHASE 4: Implementation + Validation** â†’ Context-aware execution + quality verification + approval gate
**PHASE 5: Finalization** â†’ Tracking + git + PHR

**Result**: Shipping-ready output with:
- âœ… Strategic AI use (not over-engineered)
- âœ… Constitutional compliance
- âœ… Realistic expectations
- âœ… Quality built-in
- âœ… Decision trail documented
